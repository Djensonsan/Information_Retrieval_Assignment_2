{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_Custom.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "5r2Dk6fJN46q",
        "TNW5LmQoN46q",
        "XtTg2ph8RFLK",
        "zIwnFbddQ7W1",
        "Ciu7lv26RMeM",
        "JxKk8_aYSmeN",
        "xh2qCF8wrJLt",
        "hS5KqwBWR5Gk"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djensonsan/Information_Retrieval_Assignment_2/blob/main/LDA_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vf5bYq4wN46n"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djensonsan/Information_Retrieval_Assignment_2/blob/main/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qMMWBoxbN46n"
      },
      "source": [
        "# Information Retrieval Assignment 2: LDA-Custom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O9fu0tr1N46o"
      },
      "source": [
        "## Runtime specs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xpH_XPVeN46o"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wGOFWAXLN46o"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bfjROIeZN46p"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fgMmV5osN46p"
      },
      "source": [
        "# Install your required packages here\n",
        "!pip install pandas numpy matplotlib fsspec gcsfs dask\n",
        "!pip install -q tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9LGEB4QvN46p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import dask.dataframe as dd\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "from ast import literal_eval\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9fY-mIsNN46p"
      },
      "source": [
        "# Mount google drive in colab:\n",
        "from google.cloud import storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FYiHhR3FN46p"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YQxySzmeN46p"
      },
      "source": [
        "# Pycharm:\n",
        "# data = pd.read_csv('data/news_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BaygE6mrN46q"
      },
      "source": [
        "# Colab:\n",
        "data = pd.read_csv('/content/drive/MyDrive/IR-Assignment-2/data/news_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5AFocwLeN46q"
      },
      "source": [
        "### Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ypqN2WARN46q"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OHOBtnBBN46q"
      },
      "source": [
        "data.head(n=43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G3DwN5s-N46q"
      },
      "source": [
        "# Use document 42 as running example\n",
        "data.loc[42, 'content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5r2Dk6fJN46q"
      },
      "source": [
        "### Keep document content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HCQzw6dYN46q"
      },
      "source": [
        "data_content = data['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzSuVSocP2HQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Tsu8WayxN46q"
      },
      "source": [
        "type(data_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZuKUMdK6N46q"
      },
      "source": [
        "data_content.head(n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TNW5LmQoN46q"
      },
      "source": [
        "### Tokenization, Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ImLzJGmWN46q"
      },
      "source": [
        "tqdm.pandas()\n",
        "# Note Jens: Might want to use Dask to speed things up. \n",
        "# When using Dask can't use tqdm as far as I know."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gVFgiPnwN46q"
      },
      "source": [
        "# There's NaN values in the dataset\n",
        "data_content.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kThYqBUaN46q"
      },
      "source": [
        "data_content.isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YZntpdQCN46q"
      },
      "source": [
        "# Tokenization\n",
        "data_content_tokenized = data_content.progress_apply(lambda x: nltk.word_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SzoGvCgYN46q"
      },
      "source": [
        "# Remove words smaller than 3 characters\n",
        "data_content_tokenized = data_content_tokenized.progress_apply(lambda x: [y for y in x if len(y)>2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "y1nWwyiiN46q"
      },
      "source": [
        "# Stemming and Lemmatization \n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "data_content_stemmed = data_content_tokenized.progress_apply(lambda x: [stemmer.stem(WordNetLemmatizer().lemmatize(y)) for y in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "l31Lhd1MN46q"
      },
      "source": [
        "# Remove Stopswords\n",
        "stop_words = set(stopwords.words('english')) \n",
        "data_content_clean = data_content_stemmed.progress_apply(lambda x: [y for y in x if not y in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ro0vE3F0N46q"
      },
      "source": [
        "data_content_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "F1u77zdEN46q"
      },
      "source": [
        "# data_content_clean contain the cleaned 'content' column of the news dataset:\n",
        "data_content_clean.to_csv('/content/drive/MyDrive/IR-Assignment-2/data/new_dataset_clean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EAY68bAbN46q"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmJio0CgRQ9"
      },
      "source": [
        "iterations = 10\r\n",
        "topicAmount = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "VU-jCNOtN46q"
      },
      "source": [
        "The following part contains our custom LDA implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nCpZ6PazN46q"
      },
      "source": [
        "data_content_clean = pd.read_csv('/content/drive/MyDrive/IR-Assignment-2/data/new_dataset_clean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vQOhcRfON46q"
      },
      "source": [
        "data_content_clean = data_content_clean['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_Oxe_0gJN46q"
      },
      "source": [
        "data_content_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1rBvxK4-dGr"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtTg2ph8RFLK"
      },
      "source": [
        "### Token Pre-processing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMZHk1NkOEh_"
      },
      "source": [
        "def get_freq_tokens(data, num_above=0, num_under=0, most_freq=0):\n",
        "  '''Will return in how many documents each token appears.\n",
        "  Args:\n",
        "    data (series): series object holding lists of tokens.\n",
        "    num_above (int): drop tokens who appear in less than num_above documents.\n",
        "    num_under (float): drop tokens who appear in more than num_under * amount of documents.\n",
        "    most_freq (int): return most_freq tokens.\n",
        "\n",
        "  Returns:\n",
        "    tokens_doc_freq (dict): dictionary with key = token and value = # documents token appears in.\n",
        "  '''\n",
        "  tokens_doc_freq = dict()\n",
        "  for row in tqdm(data, \"Creating Freq. Dict: \"):\n",
        "    doc_words = literal_eval(row)\n",
        "    doc_words = set(doc_words)\n",
        "    for word in doc_words:\n",
        "      if word in tokens_doc_freq:\n",
        "        tokens_doc_freq[word] += 1\n",
        "      else:\n",
        "        tokens_doc_freq[word] = 1\n",
        "  \n",
        "  if num_above and num_under:\n",
        "    number_of_documents = len(data)\n",
        "    tokens_doc_freq = {k: v for k,v in tokens_doc_freq.items() if v > num_above and v < number_of_documents*num_under}\n",
        "  elif num_above:\n",
        "    tokens_doc_freq = {k: v for k,v in tokens_doc_freq.items() if v > num_above}\n",
        "  elif num_under:\n",
        "    number_of_documents = len(data)\n",
        "    tokens_doc_freq = {k: v for k,v in tokens_doc_freq.items() if v < number_of_documents*num_under}\n",
        "  if sorted:\n",
        "    tokens_doc_freq = OrderedDict(sorted(tokens_doc_freq.items(), key=lambda x: x[1], reverse=True)[:most_freq])\n",
        "  return tokens_doc_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz0-pV0kQQV4"
      },
      "source": [
        "tokens_doc_freq = get_freq_tokens(data_content_clean, num_above=15, num_under=0.5, most_freq=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxNDr3BQRzZ3"
      },
      "source": [
        "tokens_doc_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwnFbddQ7W1"
      },
      "source": [
        "#### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uHalg9MQ8_z"
      },
      "source": [
        "len(tokens_doc_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv4gePijQ_DB"
      },
      "source": [
        "tokens_doc_freq['trump']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciu7lv26RMeM"
      },
      "source": [
        "### BOW Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNt_0drERRuz"
      },
      "source": [
        "def create_bow(data, tokens):\n",
        "  ''' Create a bag of words for usage in LDA.\n",
        "  Args:\n",
        "    data (series): series object holding lists of tokens.\n",
        "    tokens (list): list of tokens to use in bag of words.\n",
        "\n",
        "  Returns:\n",
        "    documents (list): bag of words, a list of dicts.\n",
        "  '''\n",
        "  documents = []\n",
        "  for row in tqdm(data, \"Creating BOW: \"):\n",
        "    doc_words = literal_eval(row)\n",
        "    doc_bag = dict()\n",
        "    for word in doc_words:\n",
        "      # check if word in tokens \n",
        "      if word in doc_bag and word in tokens:\n",
        "        doc_bag[word] += 1\n",
        "      elif word in tokens:\n",
        "        doc_bag[word] = 1\n",
        "    documents.append(doc_bag)\n",
        "  return documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfDtsVnySTcJ"
      },
      "source": [
        "documents = create_bow(data_content_clean, tokens_doc_freq.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxKk8_aYSmeN"
      },
      "source": [
        "#### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLYRXRaVUVzf"
      },
      "source": [
        "len(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI2QNgZ0Sl9Z"
      },
      "source": [
        "# First document has 8 occurences of Trump (same as with library)\n",
        "documents[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh2qCF8wrJLt"
      },
      "source": [
        "### Word Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hkXuoYpHRiG"
      },
      "source": [
        "class WordEncoder():\n",
        "  def __init__(self):\n",
        "    self.id_word_encoding = {}\n",
        "    self.word_id_encoding = {}\n",
        "\n",
        "  def encode(self, documents):\n",
        "    ''' Encode the words as integers.\n",
        "        Args:\n",
        "          documents (list): bag of words, a list of dicts\n",
        "\n",
        "        Returns:\n",
        "          dummy (list): encoded bag of words, a list of dicts\n",
        "    '''\n",
        "    dummy = deepcopy(documents)\n",
        "    word_id = 0\n",
        "    tokens = []\n",
        "    for doc in documents:\n",
        "      tokens.extend(doc.keys())\n",
        "    tokens_uq = set(tokens)\n",
        "    del tokens\n",
        "    for token in tokens_uq:\n",
        "      self.id_word_encoding[word_id] = token\n",
        "      self.word_id_encoding[token] = word_id\n",
        "      word_id += 1\n",
        "    del tokens_uq\n",
        "    for index, doc in enumerate(tqdm(documents, \"Encoding: \")):\n",
        "        for word in doc.keys():\n",
        "          word_id = self.word_id_encoding[word]\n",
        "          word_freq = dummy[index].pop(word)\n",
        "          dummy[index][word_id] = word_freq\n",
        "    return dummy\n",
        "  \n",
        "  def decode(self, documents):\n",
        "    ''' Decode the integers to words.\n",
        "        Args:\n",
        "          documents (list): encoded bag of words, a list of dicts\n",
        "\n",
        "        Returns:\n",
        "          dummy (list): decoded bag of words, a list of dicts\n",
        "    '''\n",
        "    dummy = deepcopy(documents)\n",
        "    word_id = 0\n",
        "    for index, doc in enumerate(tqdm(documents, \"Decoding: \")):\n",
        "      for word_id in doc.keys():\n",
        "        word = self.id_word_encoding[word_id]\n",
        "        word_freq = dummy[index].pop(word_id)\n",
        "        dummy[index][word] = word_freq\n",
        "\n",
        "  def decodeWord(self, word_id):\n",
        "    return self.id_word_encoding[word_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS5KqwBWR5Gk"
      },
      "source": [
        "#### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5uEESgCR83L"
      },
      "source": [
        "encoder = WordEncoder()\n",
        "encoded_documents = encoder.encode(documents)\n",
        "decoded_documents = encoder.decode(encoded_documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K9v-ei-QQKa"
      },
      "source": [
        "decoded_documents == documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hizRVL-nS9td"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qlHPf-ARjMf"
      },
      "source": [
        "class LDAmodel:\r\n",
        "      def __init__(self, documents, vocabulary, topics=topicAmount):\r\n",
        "        documents_len = len(documents)\r\n",
        "        # We'll only us the x most frequent tokens\r\n",
        "        self.vocabulary_len = len(tokens_doc_freq.keys())\r\n",
        "        self.beta = 1 / topics\r\n",
        "        self.alfa = 1 / topics\r\n",
        "        # number of assignments to topic k in document i\r\n",
        "        self.n_i_k = np.zeros((documents_len, topics))\r\n",
        "        # number of assignments, corpus wide, of word w to topic k\r\n",
        "        self.m_w_k = np.zeros((topics, self.vocabulary_len))\r\n",
        "        # will hold number of words in each document\r\n",
        "        self.n_d = np.zeros((documents_len))\r\n",
        "        # number of assignments to topic\r\n",
        "        self.n_z = np.zeros((topics))\r\n",
        "\r\n",
        "        # z will hold the topic matrix\r\n",
        "        self.z = [[0 for _ in range(len(doc))] for doc in documents]\r\n",
        "\r\n",
        "        self.topics = topics\r\n",
        "        self.documents = documents\r\n",
        "\r\n",
        "        for doc_id, doc in enumerate(tqdm(self.documents, \"Initializing: \")):\r\n",
        "          for word_id, word in enumerate(doc.keys()):\r\n",
        "            self.z[doc_id][word_id] = random.randrange(0, self.topics, 1)\r\n",
        "            word_topic = self.z[doc_id][word_id]\r\n",
        "            # number of assignments of topic: word_topic in document: doc_id\r\n",
        "            self.n_i_k[doc_id][word_topic] += 1\r\n",
        "            # number of global assigments of word: word to topic: word_topic\r\n",
        "            self.m_w_k[word_topic, word] += 1\r\n",
        "            # total number of word assignments to topic\r\n",
        "            self.n_z[word_topic] += 1\r\n",
        "            # total number of words in document\r\n",
        "            self.n_d[doc_id] += 1\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      def runLDA(self, iterations=10):\r\n",
        "        for iteration in tqdm(range(iterations), \"Optimizing: \"):\r\n",
        "          for doc_id, doc in enumerate(self.documents):\r\n",
        "            for word_id, word in enumerate(doc.keys()):\r\n",
        "                    # get the topic for word n in document m\r\n",
        "                    word_topic = self.z[doc_id][word_id]\r\n",
        "                    \r\n",
        "                    # decrement counts for word w with associated topic z\r\n",
        "                    self.n_i_k[doc_id][word_topic] -= 1 \r\n",
        "                    self.m_w_k[word_topic][word] -= 1\r\n",
        "                    self.n_z[word_topic] -= 1\r\n",
        "\r\n",
        "                    # sample new topic from a multinomial according to our formula\r\n",
        "                    p_d_t = (self.n_i_k[doc_id] + self.alfa) / (self.n_d[doc_id] - 1 + self.topics * self.alfa) \r\n",
        "                    p_t_w = (self.m_w_k[:, word] + self.beta) / (self.n_z + self.vocabulary_len * self.beta)\r\n",
        "                    p_z = p_d_t * p_t_w\r\n",
        "                    p_z /= np.sum(p_z)\r\n",
        "                    new_z = np.random.multinomial(1, p_z).argmax()\r\n",
        "\r\n",
        "                    # set z as the new topic and increment counts\r\n",
        "                    self.z[doc_id][word_id] = new_z\r\n",
        "                    self.n_i_k[doc_id][new_z] += 1\r\n",
        "                    self.m_w_k[new_z][word] += 1\r\n",
        "                    self.n_z[new_z] += 1\r\n",
        "\r\n",
        "      def getTopicsPerDocument(self):\r\n",
        "        return self.n_i_k\r\n",
        "\r\n",
        "      def getWordsPerTopic(self):\r\n",
        "        return self.m_w_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DP63bNMTlxL"
      },
      "source": [
        "tokens_doc_freq = get_freq_tokens(data_content_clean, num_above=15, num_under=0.5, most_freq=10000)\n",
        "documents = create_bow(data_content_clean, tokens_doc_freq.keys())\n",
        "encoder = WordEncoder()\n",
        "encoded_documents = encoder.encode(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-xU_0bco2_J"
      },
      "source": [
        "LDA = LDAmodel(encoded_documents, tokens_doc_freq, topics=20)\r\n",
        "LDA.runLDA()\r\n",
        "\r\n",
        "m_w_k = LDA.getWordsPerTopic()\r\n",
        "n_i_k = LDA.getTopicsPerDocument()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eXR1VvlAN46r"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EXsyNCmUweN"
      },
      "source": [
        "topicList = [\"Topic \"+str(i) for i in range(20)]\r\n",
        "\r\n",
        "df = pd.DataFrame(columns=topicList)\r\n",
        "\r\n",
        "for i in range(20):\r\n",
        "  indices = np.argpartition(m_w_k[i],-10)[-10:]\r\n",
        "\r\n",
        "  min_elements = m_w_k[i][indices]\r\n",
        "  min_elements_order = np.argsort(-min_elements)\r\n",
        "  indices = indices[min_elements_order]\r\n",
        "  topic = \"\"\r\n",
        "  topTopicWords = []\r\n",
        "  for ind in indices:\r\n",
        "    topic = topic + str(m_w_k[i][ind]/np.sum(m_w_k[i])) + \"*\\\"\" + encoder.decodeWord(ind) + \"\\\"  \"\r\n",
        "    topTopicWords.append(encoder.decodeWord(ind))\r\n",
        "  df[\"Topic \"+ str(i)] = topTopicWords\r\n",
        "  print('Topic: {} \\nWords: {}\\n\\n'.format(i, topic))\r\n",
        "\r\n",
        "\r\n",
        "df.to_csv('/content/drive/MyDrive/IR-Assignment-2/data/analysis_topics'+ str(iterations) +'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el1XH1P-Y-8l"
      },
      "source": [
        "# Decide the most representative documents for each topic and write it to file\r\n",
        "\r\n",
        "repList = [[] for i in range(20)]\r\n",
        "\r\n",
        "for doc in tqdm(range(len(n_i_k)), \"Scoring: \"):\r\n",
        "  if sum(n_i_k[doc]) != 0:\r\n",
        "    scores = n_i_k[doc]/sum(n_i_k[doc])\r\n",
        "\r\n",
        "  for topic in range(len(scores)):\r\n",
        "    if scores[topic] > 0:\r\n",
        "      repList[topic].append((doc, scores[topic]))\r\n",
        "\r\n",
        "for topic in repList:\r\n",
        "  topic.sort(key=lambda x:x[1], reverse=True)\r\n",
        "print(repList[0])\r\n",
        "\r\n",
        "topicList = [\"Topic \"+str(i) for i in range(20)]\r\n",
        "\r\n",
        "df = pd.DataFrame(columns=topicList)\r\n",
        "\r\n",
        "for topic in range(len(topicList)):\r\n",
        "  end = 100\r\n",
        "  if len(repList[topic]) < 100:\r\n",
        "    end = len(repList[topic])-1\r\n",
        "  df[\"Topic \"+ str(topic)] = [x[0] for x in repList[topic][0:end]]\r\n",
        "\r\n",
        "print(df)\r\n",
        "\r\n",
        "df.to_csv('/content/drive/MyDrive/IR-Assignment-2/data/topic_document_rank_custom'+ str(iterations) +'.csv')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBxKEAD23-XO"
      },
      "source": [
        "### see how divided a document is between topics and write information to file:\r\n",
        "\r\n",
        "file = open(\"/content/drive/MyDrive/IR-Assignment-2/data/analysis_topic_per_doc\"+ str(iterations) +\".txt\",\"w\") \r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  scores = n_i_k[i]/sum(n_i_k[i])\r\n",
        "  file.write(\"doc \" + str(i) + \" topics: \\n\")\r\n",
        "  for score in sorted(scores, reverse=True):\r\n",
        "    file.write(str(score) + \"   \")\r\n",
        "  file.write(\"\\n\\n\")\r\n",
        "\r\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}